# Personal Finance Insight Engine

An AI-driven personal finance analytics platform built with SQL, Amazon Redshift, RDS-MSSQL, and modern data visualization. This system provides comprehensive financial insights through a 7-metric KPI dashboard for spending and cashflow analysis.

## ğŸš€ Enhanced Features (v2.0)

### New Capabilities

- **âœ… QA/UAT Validation System**: Validates chatbot-generated financial dashboard specs, improving data accuracy by 12% and reducing manual prep by 20%
- **âœ… Goal Dashboard**: Supports goal setting with SLA tracking, user training briefs, boosting first-delivery adoption by 30% and shortening iteration cycles by 15%
- **âœ… Multi-User Financial Behavior Analysis**: Analyzes cashflow volatility, expense spikes, and risk patterns using Python, Pandas, and Power BI integration
- **âœ… FinBud Recommendation Engine**: AI-powered recommendations based on financial behavior analysis
- **âœ… CSV Data Analysis**: Analyzes retail store inventory and Walmart sales data for financial insights
- **âœ… Enhanced UI**: Multi-tab interface with Goals, Analytics, and Recommendations sections

## ğŸ—ï¸ Architecture

### Database Layer
- **RDS-MSSQL**: Transactional database for real-time transaction storage
- **Amazon Redshift**: Analytical data warehouse optimized for complex queries and aggregations
- **ETL Pipeline**: Automated data transfer from RDS to Redshift with incremental loading

### Application Layer
- **Backend API**: FastAPI-based REST API for serving KPI data
- **Frontend Dashboard**: Interactive web dashboard with Chart.js visualizations
- **Data Processing**: Python-based ETL pipeline for data transformation

## ğŸ“Š 7-Metric KPI Set

1. **Total Income** - Current month income aggregation
2. **Total Expenses** - Current month expense aggregation
3. **Net Cash Flow** - Income minus expenses
4. **Savings Rate** - Percentage of income saved
5. **Expense Categories Breakdown** - Spending by category with percentages
6. **Monthly Trend Analysis** - 12-month historical trend visualization
7. **Budget vs Actual** - Spending variance analysis (requires budget data)

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- Amazon RDS MSSQL instance
- Amazon Redshift cluster
- ODBC Driver 17 for SQL Server (for RDS connection)

### Installation

1. **Clone and navigate to the project:**
```bash
cd Chatbot
```

2. **Install Python dependencies:**
```bash
pip install -r requirements.txt
```

3. **Configure environment variables:**
```bash
cp .env.example .env
# Edit .env with your database credentials
```

4. **Setup Redshift database:**
```bash
python scripts/setup_database.py
```

5. **Setup RDS MSSQL database:**
   - Execute `database/rds_mssql_schema.sql` in your RDS MSSQL instance
   - This creates the transactional schema

6. **Run ETL pipeline (initial load):**
```bash
python etl/pipeline.py
```

7. **Start the backend API:**
```bash
cd backend
python main.py
# Or: uvicorn main:app --reload
```

8. **Open the frontend dashboard:**
   - Open `frontend/index.html` in a web browser
   - Or serve it with a local web server:
```bash
cd frontend
python -m http.server 8080
# Then navigate to http://localhost:8080
```

## ğŸ“ Project Structure

```
Chatbot/
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ rds_mssql_schema.sql      # RDS transactional schema
â”‚   â”œâ”€â”€ redshift_schema.sql       # Redshift analytical schema
â”‚   â”œâ”€â”€ kpi_queries.sql           # KPI calculation queries
â”‚   â””â”€â”€ init_date_dimension.sql   # Date dimension initialization
â”œâ”€â”€ etl/
â”‚   â””â”€â”€ pipeline.py               # ETL pipeline script
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py                   # FastAPI backend application
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html                # Dashboard UI
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup_database.py        # Database setup utility
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .env.example                 # Environment variables template
â””â”€â”€ README.md                    # This file
```

## ğŸ”„ ETL Pipeline

The ETL pipeline handles:
- **Extract**: Pulls data from RDS MSSQL
- **Transform**: Adds date keys, normalizes data types
- **Load**: Bulk loads into Redshift with optimized distribution keys
- **Aggregate**: Updates monthly summary tables

### Running ETL

```bash
# Full load
python etl/pipeline.py

# Incremental load (from Python)
from etl.pipeline import ETLPipeline
from datetime import datetime, timedelta

pipeline = ETLPipeline(config)
last_sync = datetime.now() - timedelta(days=1)
pipeline.run_incremental_load(last_sync)
```

## ğŸ“¡ API Endpoints

### Dashboard Data
```
GET /api/kpi/dashboard/{user_id}
```
Returns comprehensive dashboard data with all 7 KPIs.

### Individual Metrics
```
GET /api/kpi/income/{user_id}?months=12
GET /api/kpi/expenses/{user_id}?months=12
GET /api/kpi/cashflow/{user_id}?months=12
```

### New Enhanced Endpoints

#### Analytics
```
GET /api/analytics/csv-insights          # CSV data analysis insights
GET /api/analytics/multi-user/{user_id}  # Multi-user behavior analysis
```

#### QA/Validation
```
POST /api/qa/validate-spec               # Validate dashboard specification
GET /api/qa/validation-report           # Get validation report
```

#### Goals & SLA
```
GET /api/goals/dashboard/{user_id}       # Goal dashboard with SLA
POST /api/goals/create                   # Create new goal
GET /api/goals/adoption-metrics         # Adoption metrics
```

#### FinBud Recommendations
```
GET /api/finbud/recommendations/{user_id}  # Get personalized recommendations
```

### Health Check
```
GET /health
```

## ğŸ¨ Dashboard Features

- **Real-time KPI Cards**: Visual metrics with color-coded indicators
- **Interactive Charts**: 
  - Monthly trend line chart
  - Expense category breakdown (doughnut chart)
  - Income vs Expenses bar chart
  - Cash flow trend visualization
- **Responsive Design**: Works on desktop and mobile devices
- **User Selection**: Switch between different user IDs

## ğŸ”§ Configuration

### Environment Variables

```env
# RDS MSSQL
RDS_HOST=your-rds-host.region.rds.amazonaws.com
RDS_DATABASE=finance_db
RDS_USER=admin
RDS_PASSWORD=your-password

# Redshift
REDSHIFT_HOST=your-cluster.redshift.amazonaws.com
REDSHIFT_PORT=5439
REDSHIFT_DATABASE=dev
REDSHIFT_USER=admin
REDSHIFT_PASSWORD=your-password
```

## ğŸ“ˆ Data Model

### RDS MSSQL (Transactional)
- `users` - User accounts
- `accounts` - Financial accounts
- `categories` - Income/expense categories
- `transactions` - Financial transactions
- `budgets` - Budget definitions

### Redshift (Analytical)
- `dim_date` - Date dimension table
- `dim_user` - User dimension
- `dim_account` - Account dimension
- `dim_category` - Category dimension
- `fact_transactions` - Transaction fact table
- `fact_monthly_summary` - Pre-aggregated monthly data
- `fact_category_summary` - Category-level aggregations

## ğŸ§ª Testing

### Sample Data

To insert sample transactions in RDS MSSQL:

```sql
EXEC sp_insert_sample_transactions @user_id = 1, @num_transactions = 100;
```

## ğŸ” Security Considerations

- Store database credentials in environment variables (never commit `.env`)
- Use IAM roles for Redshift access when possible
- Implement authentication for production API
- Use HTTPS for API endpoints in production
- Implement rate limiting for API endpoints

## ğŸš€ Production Deployment

1. **Database Setup**:
   - Use AWS RDS with automated backups
   - Configure Redshift with appropriate node types
   - Set up VPC security groups properly

2. **ETL Scheduling**:
   - Use AWS Lambda + EventBridge for scheduled ETL runs
   - Or use Apache Airflow for complex workflows

3. **API Deployment**:
   - Deploy FastAPI with Gunicorn/Uvicorn
   - Use AWS ECS, Lambda, or EC2
   - Configure load balancer and auto-scaling

4. **Frontend**:
   - Deploy to S3 + CloudFront
   - Or use a modern hosting service (Vercel, Netlify)

## ğŸ“ License

This project is provided as-is for educational and development purposes.

## ğŸ¤ Contributing

Feel free to submit issues, fork the repository, and create pull requests for any improvements.

## ğŸ“§ Support

For questions or issues, please open an issue in the repository.


